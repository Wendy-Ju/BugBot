{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b45ffaa-4f99-4144-93b8-74cdee700fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 11m 25s]\n",
      "val_accuracy: 0.4545454680919647\n",
      "\n",
      "Best val_accuracy So Far: 0.4840908944606781\n",
      "Total elapsed time: 03h 55m 35s\n",
      "Epoch 1/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 323ms/step - accuracy: 0.1330 - loss: 2.3624 - val_accuracy: 0.2705 - val_loss: 2.1991\n",
      "Epoch 2/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 341ms/step - accuracy: 0.2729 - loss: 2.1061 - val_accuracy: 0.3318 - val_loss: 2.0641\n",
      "Epoch 3/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 345ms/step - accuracy: 0.3395 - loss: 1.9457 - val_accuracy: 0.3886 - val_loss: 1.9840\n",
      "Epoch 4/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 326ms/step - accuracy: 0.4189 - loss: 1.7702 - val_accuracy: 0.4000 - val_loss: 1.9555\n",
      "Epoch 5/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 323ms/step - accuracy: 0.4485 - loss: 1.6328 - val_accuracy: 0.4114 - val_loss: 1.9632\n",
      "Epoch 6/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 323ms/step - accuracy: 0.5177 - loss: 1.4669 - val_accuracy: 0.4114 - val_loss: 1.9182\n",
      "Epoch 7/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 324ms/step - accuracy: 0.5725 - loss: 1.3268 - val_accuracy: 0.4250 - val_loss: 1.9634\n",
      "Epoch 8/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 325ms/step - accuracy: 0.6031 - loss: 1.2122 - val_accuracy: 0.4182 - val_loss: 1.9799\n",
      "Epoch 9/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 324ms/step - accuracy: 0.6593 - loss: 1.0651 - val_accuracy: 0.4705 - val_loss: 2.0066\n",
      "Epoch 10/10\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 325ms/step - accuracy: 0.6966 - loss: 0.9509 - val_accuracy: 0.4614 - val_loss: 2.0095\n",
      "best parameters:\n",
      " {'best_lr': 0.0001, 'best_dropout': 0.30000000000000004, 'best_batch_size': 16}\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.3438 - loss: 2.2868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keeganveazey/Desktop/ds_capstone/BugBot/bugbot_env/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.3424 - loss: 2.2733 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 2.2131574153900146, test_acc: 0.36818182468414307\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0, VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperParameters \n",
    "\n",
    "\n",
    "# ----------- CONSTANTS ----------------\n",
    "\n",
    "# define directory structure\n",
    "TRAIN_DIR = \"PROCESSED_DATA/TRAINING_DATA/TRAINING_AUGMENTED_DATA\"\n",
    "VALID_DIR = \"PROCESSED_DATA/VALIDATION_DATA/\"\n",
    "TEST_DIR = \"PROCESSED_DATA/TEST_DATA/\"\n",
    "\n",
    "# Image Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NORMALIZE_FLAG = True\n",
    "NO_FRILLS_DATAGEN = ImageDataGenerator()\n",
    "NORM_DATAGEN = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "TRAIN_GENERATOR = load_data(TRAIN_DIR)\n",
    "VAL_GENERATOR = load_data(VALID_DIR)\n",
    "TEST_GENERATOR = load_data(TEST_DIR,shuffle_flag=False)\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "def load_data(directory,shuffle_flag=True):\n",
    "    '''\n",
    "    Param: \n",
    "        - directory - str, \n",
    "        - shuffle_flag - boolean, introduces constrolled stochasticity\n",
    "    '''\n",
    "    if NORMALIZE_FLAG == True:\n",
    "        generator = NORM_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "    else:\n",
    "        generator = NO_FRILLS_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "\n",
    "    \n",
    "# MODEL BUILDING Functions -------------------\n",
    "\n",
    "def build_tunable_cnn(hp):\n",
    "    '''\n",
    "    Params: hp - keras HyperParameter object\n",
    "    Use: Builds cnn with tunable hyperparameter for use with bayesian optimization\n",
    "    '''\n",
    "    model = Sequential([\n",
    "        Input(shape=(224, 224, 3)),\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)), #tune dropout\n",
    "        Dense(TRAIN_GENERATOR.num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # tune learning rate, batch size\n",
    "    learning_rate = hp.Choice('lr', values=[1e-2, 1e-3, 1e-4])\n",
    "    batch_size = hp.Choice('batch_size', values=[16, 32])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def build_best_model():\n",
    "    '''Builds cnn model from best performing hyperparameters'''\n",
    "    \n",
    "    # us keras bayesian tuner\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_tunable_cnn,\n",
    "        objective='val_accuracy',  # tune by improving validation accuracy\n",
    "        max_trials=20,  # num different hp combos to try\n",
    "        executions_per_trial=1,  # run each model once\n",
    "        directory='bayesian_tuning',\n",
    "        project_name='lr_and_drop_tuning'\n",
    "    )\n",
    "    \n",
    "    # search hp combos\n",
    "    tuner.search(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10)\n",
    "    \n",
    "    # get best hps\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # save them\n",
    "    best_hps_dict = {'best_lr': best_hps.get('lr'),\n",
    "                    'best_dropout': best_hps.get('dropout'),\n",
    "                    'best_batch_size': best_hps.get('batch_size')}\n",
    "    \n",
    "    # make final model with the best drop out, learning rate and batch size\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model_training_history = best_model.fit(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10, batch_size=best_hps.get('batch_size'))\n",
    "\n",
    "    return best_hps_dict, best_model, best_model_training_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, filename = \"best_model.h5\"):\n",
    "    '''\n",
    "    Saves model to h5 file, returns test accuracy loss and test accuracy\n",
    "    '''\n",
    "    # evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(TEST_GENERATOR)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # save to file\n",
    "    model.save(filename)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # best model\n",
    "    best_hps_dict, best_model, best_model_training_history = build_best_model()\n",
    "\n",
    "    print(f'best parameters:\\n {best_hps_dict}')\n",
    "    \n",
    "    test_loss, test_acc = evaluate_model(best_model)\n",
    "    print(f'test_loss: {test_loss}, test_acc: {test_acc}')\n",
    "    \n",
    "    best_model.save(\"simple_cnn_best_model_bayes_optimization.h5\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

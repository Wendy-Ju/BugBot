{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45ffaa-4f99-4144-93b8-74cdee700fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6642 images belonging to 11 classes.\n",
      "Found 440 images belonging to 11 classes.\n",
      "Found 220 images belonging to 11 classes.\n",
      "Reloading Tuner from bayesian_tuning/lr_and_drop_tuning/tuner0.json\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.2               |0.4               |dropout\n",
      "0.01              |0.0001            |lr\n",
      "32                |16                |batch_size\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m 51/208\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 349ms/step - accuracy: 0.1069 - loss: 18.7145 "
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0, VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperParameters \n",
    "\n",
    "\n",
    "# ----------- CONSTANTS ----------------\n",
    "\n",
    "# define directory structure\n",
    "TRAIN_DIR = \"PROCESSED_DATA/TRAINING_DATA/TRAINING_AUGMENTED_DATA\"\n",
    "VALID_DIR = \"PROCESSED_DATA/VALIDATION_DATA/\"\n",
    "TEST_DIR = \"PROCESSED_DATA/TEST_DATA/\"\n",
    "\n",
    "# Image Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NORMALIZE_FLAG = True\n",
    "NO_FRILLS_DATAGEN = ImageDataGenerator()\n",
    "NORM_DATAGEN = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "TRAIN_GENERATOR = load_data(TRAIN_DIR)\n",
    "VAL_GENERATOR = load_data(VALID_DIR)\n",
    "TEST_GENERATOR = load_data(TEST_DIR,shuffle_flag=False)\n",
    "hp = HyperParameters()\n",
    "\n",
    "\n",
    "def load_data(directory,shuffle_flag=True):\n",
    "    '''\n",
    "    Param: \n",
    "        - directory - str, \n",
    "        - shuffle_flag - boolean, introduces constrolled stochasticity\n",
    "    '''\n",
    "    if NORMALIZE_FLAG == True:\n",
    "        generator = NORM_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "    else:\n",
    "        generator = NO_FRILLS_DATAGEN.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',shuffle=shuffle_flag)\n",
    "        return generator\n",
    "\n",
    "    \n",
    "# MODEL BUILDING Functions -------------------\n",
    "\n",
    "def build_tunable_cnn(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(224, 224, 3)),\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.5, step=0.1)), #tune dropout\n",
    "        Dense(TRAIN_GENERATOR.num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # tune learning rate\n",
    "    learning_rate = hp.Choice('lr', values=[1e-2, 1e-3, 1e-4])\n",
    "    batch_size = hp.Choice('batch_size', values=[16, 32])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def build_best_model():\n",
    "    \n",
    "    # Define the Bayesian tuner\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_tunable_cnn,\n",
    "        objective='val_accuracy',  # tune by improving validation accuracy\n",
    "        max_trials=20,  # num different hp combos to try\n",
    "        executions_per_trial=1,  # run each model once\n",
    "        directory='bayesian_tuning',\n",
    "        project_name='lr_and_drop_tuning'\n",
    "    )\n",
    "    \n",
    "    # random search hp combos\n",
    "    tuner.search(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10)\n",
    "    \n",
    "    # get best hps\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # save them\n",
    "    best_hps_dict = {'best_lr': best_hps.get('lr'),\n",
    "                    'best_dropout': best_hps.get('dropout'),\n",
    "                    'best_batch_size': best_hps.get('batch_size')}\n",
    "    \n",
    "    # make final model with the best drop out, learning rate and batch size\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    best_model_training_history = best_model.fit(TRAIN_GENERATOR, validation_data=VAL_GENERATOR, epochs=10, batch_size=best_hps.get('batch_size'))\n",
    "\n",
    "    return best_hps_dict, best_model, best_model_training_history\n",
    "\n",
    "\n",
    "def evaluate_model(model, filename = \"best_model.h5\"):\n",
    "    '''\n",
    "    Saves model to h5 file, returns test accuracy loss and test accuracy\n",
    "    '''\n",
    "    # evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(TEST_GENERATOR)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # save to file\n",
    "    model.save(filename)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # best model\n",
    "    best_hps_dict, best_model, best_model_training_history = build_best_model()\n",
    "\n",
    "    print(f'best parameters:\\n {best_hps_dict}')\n",
    "    \n",
    "    test_loss, test_acc = evaluate_model(best_model)\n",
    "    print(f'test_loss: {test_loss}, test_acc: {test_acc}')\n",
    "    \n",
    "    best_model.save(\"simple_cnn_best_model_bayes_optimization.h5\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

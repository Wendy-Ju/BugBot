{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Downloading 5 images of 'house centipede' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/house_centipede_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'silverfish' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/silverfish_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'bedbug' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/bedbug_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'fleas' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/fleas_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'ticks' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/ticks_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'carpenter ant' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/carpenter_ant_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'american house spider' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/american_house_spider_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'cellar spider' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/cellar_spider_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'brown stink bug' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/brown_stink_bug_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'rice weevil' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/rice_weevil_urls.csv'.\n",
      "\n",
      "=== Downloading 5 images of 'subterranean termite' ===\n",
      "[INFO] Renamed images and saved URLs to 'DATA/CSV/subterranean_termite_urls.csv'.\n",
      "\n",
      "All downloads and CSV exports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# FILE DESCRIPTION: -------------------------------------------------------\n",
    "\n",
    "# This file web scrapes images via Bing. Includes two functions that captures the log as a string, separates invalid\n",
    "# and valid URLs, renames downloaded images in the order they were saved, and writes [image_file_name, URL] to a CSV.\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ----------- IMPORTS ----------------\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bing_image_downloader import downloader\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "\n",
    "# ----------- CONSTANTS ----------------\n",
    "\n",
    "# Image download\n",
    "IMAGE_LIMIT = 150  \n",
    "TIMEOUT = 60  \n",
    "\n",
    "# Query dictionary\n",
    "QUERIES = {\n",
    "    \"house centipede\": \"house_centipede_urls.csv\",\n",
    "    \"silverfish\": \"silverfish_urls.csv\",\n",
    "    \"bedbug\": \"bedbug_urls.csv\",\n",
    "    \"fleas\": \"fleas_urls.csv\",\n",
    "    \"ticks\": \"ticks_urls.csv\",\n",
    "    \"carpenter ant\": \"carpenter_ant_urls.csv\",\n",
    "    \"american house spider\": \"american_house_spider_urls.csv\",\n",
    "    \"cellar spider\": \"cellar_spider_urls.csv\",\n",
    "    \"brown stink bug\": \"brown_stink_bug_urls.csv\",\n",
    "    \"rice weevil\": \"rice_weevil_urls.csv\",\n",
    "    \"subterranean termite\": \"subterranean_termite_urls.csv\"\n",
    "}\n",
    "\n",
    "# Directory structure\n",
    "OUTPUT_DIR = \"DATA/IMAGES\"  \n",
    "CSV_DIR = \"DATA/CSV\"  \n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def process_download_logs(logs_str: str, download_folder: str, csv_file: str, rename_suffix: str) -> None:\n",
    "    \"\"\"\n",
    "    Parses the Bing Image Downloader logs to extract valid URLs, \n",
    "    renames images, and saves URLs to a CSV\n",
    "    \n",
    "    Parameters:\n",
    "        logs_str (str) - captured log as a string\n",
    "        download_folder (str) - directory where images are downloaded\n",
    "        csv_file (str) -  path to save CSV files\n",
    "        rename_suffix (str) - suffix for renaming images\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Error check: checking for correct params (strings) before contuining \n",
    "    if not isinstance(logs_str, str) or not isinstance(download_folder, str) \\\n",
    "       or not isinstance(csv_file, str) or not isinstance(rename_suffix, str):\n",
    "        raise TypeError(\"All parameters must be strings.\")\n",
    "\n",
    "    # Splits the logs into lines\n",
    "    logs = logs_str.splitlines()\n",
    "\n",
    "    # Lists to store valid and invalid image URLs\n",
    "    valid_urls = []\n",
    "    invalid_urls = set()\n",
    "\n",
    "    # Identify invalid URLs in the logs to filter them out later\n",
    "    for line in logs:\n",
    "        if \"[Error]Invalid image\" in line or \"Issue getting:\" in line:\n",
    "            url_start = line.find(\"http\")\n",
    "            if url_start != -1:\n",
    "                invalid_urls.add(line[url_start:].strip())\n",
    "\n",
    "     # Retrieves the valid URLs by identifying the invalid URLs\n",
    "    for line in logs:\n",
    "        if \"http\" in line:\n",
    "            url_start = line.find(\"http\")\n",
    "            if url_start != -1:\n",
    "                clean_url = line[url_start:].strip()\n",
    "                if clean_url not in invalid_urls:\n",
    "                    valid_urls.append(clean_url)\n",
    "\n",
    "    # Error check: checks if download folder exists before continuing \n",
    "    if not os.path.exists(download_folder):\n",
    "        print(f\"[ERROR] Download folder '{download_folder}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Sort files by creation time so earliest are considered first\n",
    "    downloaded_files = sorted(\n",
    "        os.listdir(download_folder),\n",
    "        key=lambda x: os.path.getctime(os.path.join(download_folder, x))\n",
    "    )\n",
    "\n",
    "    # Dataframe to store image filenames and URLs\n",
    "    url_df = pd.DataFrame(columns=[\"image file name\", \"URL\"])\n",
    "\n",
    "    # Rename files in order and updates the dataframe\n",
    "    for index, (filename, url) in enumerate(zip(downloaded_files, valid_urls), start=1):\n",
    "        file_extension = os.path.splitext(filename)[1]\n",
    "        new_name = f\"image_{index}_{rename_suffix}{file_extension}\"\n",
    "\n",
    "        os.rename(\n",
    "            os.path.join(download_folder, filename),\n",
    "            os.path.join(download_folder, new_name)\n",
    "        )\n",
    "        url_df.loc[index] = [new_name, url]\n",
    "    \n",
    "    # Saves dataframe to CSV file\n",
    "    url_df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[INFO] Renamed images and saved URLs to '{csv_file}'.\")\n",
    "\n",
    "\n",
    "def download_images_and_capture_logs(query: str, limit: int = IMAGE_LIMIT, output_dir: str = OUTPUT_DIR,\n",
    "                                     adult_filter_off: bool = True, force_replace: bool = False,\n",
    "                                     timeout: int = TIMEOUT) -> str:\n",
    "    \"\"\"\n",
    "    Downloads images using Bing Image Downloader and captures logs as a string\n",
    "    \n",
    "    Parameters:\n",
    "        query (str) -  search query\n",
    "        limit (int) -  amount of images to download\n",
    "        output_dir (str) -  directory to save images\n",
    "        adult_filter_off (bool) -  turns off adult filter\n",
    "        force_replace (bool) -  overwrites existing images\n",
    "        timeout (int) -  timeout value (in seconds) for downloading images\n",
    "    \n",
    "    Returns:\n",
    "        (str) - captured logs from the downloader as a string\n",
    "    \"\"\"\n",
    "\n",
    "    # Error check: checking for correct params (strings) before contuining \n",
    "    if not isinstance(query, str) or not isinstance(limit, int) or not isinstance(output_dir, str):\n",
    "        raise TypeError(\"Invalid parameter types.\")\n",
    "    \n",
    "    # Capture logs from downloader\n",
    "    buffer = io.StringIO()\n",
    "    with redirect_stdout(buffer):\n",
    "        downloader.download(\n",
    "            query=query,\n",
    "            limit=limit,\n",
    "            output_dir=output_dir,\n",
    "            adult_filter_off=adult_filter_off,\n",
    "            force_replace=force_replace,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    return buffer.getvalue()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the image downloads and log processing functions\n",
    "    \"\"\"\n",
    "    # Go over each insect query and corresponding CSV filename\n",
    "    for query, csv_filename in QUERIES.items():\n",
    "        print(f\"\\n=== Downloading {IMAGE_LIMIT} images of '{query}' ===\")\n",
    "\n",
    "        # 1) download images and capture the logs\n",
    "        logs = download_images_and_capture_logs(query=query)\n",
    "        \n",
    "        # 2) define file paths for downloaded imagews and CSV file\n",
    "        download_folder = os.path.join(OUTPUT_DIR, query)\n",
    "        csv_file = os.path.join(CSV_DIR, csv_filename)\n",
    "\n",
    "        # 3) processes logs, renames images, and saves URLs to a CSV file\n",
    "        process_download_logs(logs, download_folder, csv_file, query)\n",
    "\n",
    "    print(\"\\nAll downloads and CSV exports completed successfully!\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # TEST CASE / EXPECTED RESULTS when this script is run:\n",
    "\n",
    "        # 11 CSV files in CSV folder (path: DATA/CSV)\n",
    "        # 11 class folders in IMAGES folder (path: DATA/IMAGES)\n",
    "        # 150 images per class in IMAGES folder (path: DATA/IMAGES)\n",
    "    \n",
    "        # time completion: around 16.5-22 minutes\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
